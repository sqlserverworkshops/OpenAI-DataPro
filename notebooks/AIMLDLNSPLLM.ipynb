{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "![](../graphics/microsoftlogo.png)\n",
                "\n",
                "# Workshop: Unlocking AI Potential for the Data Professional - Azure OpenAI\n",
                "\n",
                "#### <i>A Microsoft Course from Microsoft Engineering and the FastTrack Team</i>\n",
                "\n",
                "<p style=\"border-bottom: 1px solid lightgrey;\"></p>\n",
                "\n",
                "<img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"https://raw.githubusercontent.com/microsoft/sqlworkshops/master/graphics/textbubble.png\"> <h2>Course Notebook: Module 1</h2>\n",
                "\n",
                "Welcome to this Microsoft solutions workshop on [*Unlocking AI Potential for the Data Professional with Azure OpenAI*](https://github.com/sqlserverworkshops/OpenAI-DataPro/tree/main). In this Notebook, you'll apply the concepts you learned in this Module.\n",
                "\n",
                "This Notebook contains recipes for some common applications of Artifiical Intelligence, Machine Learning, Deep Learning, Natural Language Processing, and Generative AI. \n",
                "\n",
                "You'll need a working knowledge of [pandas](http://pandas.pydata.org/), [matplotlib](http://matplotlib.org/), [numpy](http://www.numpy.org/), and, of course, [scikit-learn](http://scikit-learn.org/stable/) to benefit from it."
            ],
            "metadata": {
                "azdata_cell_guid": "247207f8-b0e9-466d-8385-16e561b12fc5"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Notebook dependencies and setups\n",
                "#%pip install --upgrade pip\n",
                "#%pip install matplotlib\n",
                "#%pip install keras\n",
                "#%pip install tensorflow\n",
                "#%pip install scipy\n",
                "#%pip install scikit-learn\n",
                "#%pip install nltk\n",
                "#%pip install gensim\n",
                "# Download the Punkt Tokenizer Models.\n",
                "nltk.download('punkt')\n",
                "%matplotlib inline"
            ],
            "metadata": {
                "collapsed": false,
                "trusted": true,
                "azdata_cell_guid": "d7809bf0-0c3f-4d0c-b713-24b4b9af3613",
                "language": "python",
                "tags": []
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Collecting gensim\n  Downloading gensim-4.3.2-cp38-cp38-win_amd64.whl.metadata (8.5 kB)\nRequirement already satisfied: numpy>=1.18.5 in c:\\users\\buck\\azuredatastudio-python\\lib\\site-packages (from gensim) (1.24.3)\nRequirement already satisfied: scipy>=1.7.0 in c:\\users\\buck\\azuredatastudio-python\\lib\\site-packages (from gensim) (1.10.1)\nCollecting smart-open>=1.8.1 (from gensim)\n  Downloading smart_open-7.0.1-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: wrapt in c:\\users\\buck\\azuredatastudio-python\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\nDownloading gensim-4.3.2-cp38-cp38-win_amd64.whl (24.0 MB)\n   ---------------------------------------- 24.0/24.0 MB 24.2 MB/s eta 0:00:00\nDownloading smart_open-7.0.1-py3-none-any.whl (60 kB)\n   ---------------------------------------- 60.8/60.8 kB 3.2 MB/s eta 0:00:00\nInstalling collected packages: smart-open, gensim\nSuccessfully installed gensim-4.3.2 smart-open-7.0.1\nNote: you may need to restart the kernel to use updated packages.\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 16
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Artificial Intelligence: Simple \"Expert System\"\n",
                "\n",
                "The Following Python code shows a simple example of a \"Good Old Fashioned AI\" (GOFAI) program in Python. This program uses a basic rule-based system to diagnose a simple medical condition based on symptoms.\n",
                "\n",
                "This code creates a rule-based system for diagnosing a medical condition based on a set of symptoms. The `RuleBasedSystem` class has a dictionary of rules, where each key is a set of symptoms and each value is a possible diagnosis. The `diagnose` method takes a set of symptoms as input and returns a diagnosis based on the rules.\n",
                "\n",
                "Please note that this is a very simplified example and not an example of modern real-world AI systems, especially those used in healthcare, which are much more complex and sophisticated. \n",
                "\n",
                "**Always consult with a healthcare professional for medical advice**"
            ],
            "metadata": {
                "azdata_cell_guid": "50099859-2d31-4e90-8c3a-491c45da7b7f"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "class RuleBasedSystem:\n",
                "    def __init__(self):\n",
                "        self.rules = {\n",
                "            'fever': 'You may have a common cold.',\n",
                "            'cough': 'You may have a common cold.',\n",
                "            'fever and cough': 'You may have the flu.',\n",
                "            'rash': 'You may have an allergic reaction.',\n",
                "            'headache': 'You may be dehydrated.',\n",
                "        }\n",
                "\n",
                "    def diagnose(self, symptoms):\n",
                "        diagnosis = self.rules.get(symptoms, 'Symptoms not recognized.')\n",
                "        return diagnosis\n",
                "\n",
                "# Create a rule-based system\n",
                "system = RuleBasedSystem()\n",
                "\n",
                "# Diagnose based on symptoms\n",
                "symptoms = 'fever and cough'\n",
                "diagnosis = system.diagnose(symptoms)\n",
                "\n",
                "print(f'Diagnosis: {diagnosis}')"
            ],
            "metadata": {
                "azdata_cell_guid": "18219f8c-bd73-4ecb-873a-28367af25c49",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Diagnosis: You may have the flu.\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 3
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Artificial Intelligence: Descision tree using simple Bayesian Decision Theory\n",
                "This is an over-simpified example of normative Bayesian decision theory in Python. This program uses Bayesian inference to update the probability estimate for a hypothesis as more evidence or information becomes available.\n",
                "\n",
                "This code creates a Bayesian decision system that updates its belief about the mean of a normal distribution given some observed data. The `BayesianDecision` class has a method `update` that takes a data array as input and updates the prior mean and standard deviation based on the data. The updated mean and standard deviation are then printed out.\n",
                "\n",
                "Please note that this is a very simplified example and real-world AI systems, especially those used in decision making, are much more complex and sophisticated. **Always consult with a professional for decision making advice**"
            ],
            "metadata": {
                "azdata_cell_guid": "8b72257b-0e84-4bc8-bda6-0d4ebea7a790"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import numpy as np\n",
                "from scipy.stats import norm\n",
                "\n",
                "class BayesianDecision:\n",
                "    def __init__(self, prior_mean, prior_std, likelihood_std):\n",
                "        self.prior_mean = prior_mean\n",
                "        self.prior_std = prior_std\n",
                "        self.likelihood_std = likelihood_std\n",
                "\n",
                "    def update(self, data):\n",
                "        # Calculate the posterior mean and standard deviation\n",
                "        likelihood_variance = self.likelihood_std ** 2\n",
                "        prior_variance = self.prior_std ** 2\n",
                "        posterior_variance = 1 / ((1 / likelihood_variance) + (1 / prior_variance))\n",
                "        posterior_mean = posterior_variance * ((self.prior_mean / prior_variance) + (np.mean(data) / likelihood_variance))\n",
                "        self.prior_mean = posterior_mean\n",
                "        self.prior_std = np.sqrt(posterior_variance)\n",
                "        return self.prior_mean, self.prior_std\n",
                "\n",
                "# Initialize a Bayesian decision system\n",
                "system = BayesianDecision(prior_mean=0, prior_std=1, likelihood_std=0.5)\n",
                "\n",
                "# Update the system with some data\n",
                "data = np.random.normal(loc=1, scale=0.5, size=100)\n",
                "posterior_mean, posterior_std = system.update(data)\n",
                "\n",
                "print(f'Posterior Mean: {posterior_mean}')\n",
                "print(f'Posterior Standard Deviation: {posterior_std}')"
            ],
            "metadata": {
                "azdata_cell_guid": "098b4ac6-ab24-4830-b413-ce711fccf7ca",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Posterior Mean: 0.8428954618373441\nPosterior Standard Deviation: 0.4472135954999579\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 7
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Machine Learning: Predicting results from data\n",
                "\n",
                "The following Python Code shows a simple example of a machine learning program using the `scikit-learn` library in Python. This program uses the `Iris` dataset to train a logistic regression model and make predictions.\n",
                "\n",
                "This code first loads the Iris dataset, which is a multivariate dataset introduced by the British statistician and biologist Ronald Fisher. It's often used as a beginner's dataset for machine learning and data visualization.\n",
                "\n",
                "The data is then split into a training set and a test set. The features are standardized to have a mean of 0 and a standard deviation of 1, which is a common requirement for many machine learning algorithms.\n",
                "\n",
                "A logistic regression model is trained on the training data, and then it makes predictions on the unseen test data. The predictions are then printed out."
            ],
            "metadata": {
                "azdata_cell_guid": "e96dc930-de0b-4b6a-9267-6b2f51dd1542"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "from sklearn import datasets\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "\n",
                "# Load Iris dataset\n",
                "iris = datasets.load_iris()\n",
                "X = iris.data\n",
                "y = iris.target\n",
                "\n",
                "# Split the data into training and test sets\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# Standardize the features\n",
                "sc = StandardScaler()\n",
                "X_train = sc.fit_transform(X_train)\n",
                "X_test = sc.transform(X_test)\n",
                "\n",
                "# Train a Logistic Regression model\n",
                "model = LogisticRegression(random_state=42)\n",
                "model.fit(X_train, y_train)\n",
                "\n",
                "# Make predictions on the test set\n",
                "predictions = model.predict(X_test)\n",
                "\n",
                "print(f'Predictions: {predictions}')"
            ],
            "metadata": {
                "azdata_cell_guid": "10fdf176-856d-454d-9607-6941e362f5de",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Predictions: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 8
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Deep Learning: Training a Model with Layers\n",
                "\n",
                "Here’s a simple example of a deep learning model using TensorFlow, a popular Python library for deep learning. This model is a 2-layer network that classifies input data into one of 10 categories. This code does the following:\n",
                "\n",
                "**Creates a sequential model:** This is a linear stack of layers that we can add to one at a time with the .add() method.\n",
                "\n",
                "**Adds the first layer:** This layer has 64 nodes and uses the ‘relu’ activation function.\n",
                "\n",
                "**Adds the second layer:** This layer also has 64 nodes and uses the ‘relu’ activation function.\n",
                "\n",
                "**Adds a softmax layer:** This layer has 10 nodes (one for each class) and uses the ‘softmax’ activation function, which makes the output a set of probabilities that sum to 1.\n",
                "\n",
                "**Compiles the model:** This step prepares the model for training. The ‘categorical\\_crossentropy’ loss function and ‘RMSprop’ optimizer are common choices for classification problems.\n",
                "\n",
                "**Generates dummy data:** The data and labels variables are arrays of 32-element and 10-element vectors, respectively, which represent the features and classes of our data.\n",
                "\n",
                "**Trains the model:** The model learns from the training data over 5 epochs (iterations over the entire dataset) in batches of 32 samples.\n",
                "\n",
                "**Evaluates the model:** The model’s performance is assessed using the same data."
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "bf1d4b43-a63f-431e-8c98-c49c15642d4a"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import tensorflow as tf\r\n",
                "from tensorflow.keras import layers\r\n",
                "\r\n",
                "# Create a sequential model\r\n",
                "model = tf.keras.Sequential()\r\n",
                "\r\n",
                "# Add a densely-connected layer with 64 units to the model:\r\n",
                "model.add(layers.Dense(64, activation='relu'))\r\n",
                "\r\n",
                "# Add another layer:\r\n",
                "model.add(layers.Dense(64, activation='relu'))\r\n",
                "\r\n",
                "# Add a softmax layer with 10 output units:\r\n",
                "model.add(layers.Dense(10, activation='softmax'))\r\n",
                "\r\n",
                "# Configure a model for categorical classification.\r\n",
                "model.compile(optimizer=tf.keras.optimizers.RMSprop(0.01),\r\n",
                "              loss=tf.keras.losses.CategoricalCrossentropy(),\r\n",
                "              metrics=['accuracy'])\r\n",
                "\r\n",
                "# Generate dummy data\r\n",
                "import numpy as np\r\n",
                "\r\n",
                "data = np.random.random((1000, 32))\r\n",
                "labels = np.random.random((1000, 10))\r\n",
                "\r\n",
                "# Train the model for 5 epochs\r\n",
                "model.fit(data, labels, epochs=5, batch_size=32)\r\n",
                "\r\n",
                "# Evaluate the model\r\n",
                "loss, accuracy = model.evaluate(data, labels, batch_size=32)\r\n",
                "\r\n",
                "# Print the loss and accuracy\r\n",
                "print(f\"Loss: {loss}\")\r\n",
                "print(f\"Accuracy: {accuracy}\")\r\n",
                "\r\n",
                ""
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "b7b862a8-dbdd-47ea-b16c-250f3296c04a"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Epoch 1/5\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\r 1/32 [..............................] - ETA: 15s - loss: 11.9864 - accuracy: 0.2812",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/32 [=========================>....] - ETA: 0s - loss: 199.1652 - accuracy: 0.1016",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r32/32 [==============================] - 1s 2ms/step - loss: 225.3774 - accuracy: 0.1000\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Epoch 2/5\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\r 1/32 [..............................] - ETA: 0s - loss: 495.4186 - accuracy: 0.1250",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r24/32 [=====================>........] - ETA: 0s - loss: 885.7545 - accuracy: 0.0990",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r32/32 [==============================] - 0s 2ms/step - loss: 997.7457 - accuracy: 0.0900\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Epoch 3/5\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\r 1/32 [..............................] - ETA: 0s - loss: 2113.9497 - accuracy: 0.1250",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/32 [============>.................] - ETA: 0s - loss: 1696.9198 - accuracy: 0.1116",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r32/32 [==============================] - 0s 3ms/step - loss: 2068.5786 - accuracy: 0.1210\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Epoch 4/5\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\r 1/32 [..............................] - ETA: 0s - loss: 1932.2970 - accuracy: 0.1250",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r32/32 [==============================] - ETA: 0s - loss: 3337.1069 - accuracy: 0.1030",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r32/32 [==============================] - 0s 2ms/step - loss: 3337.1069 - accuracy: 0.1030\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Epoch 5/5\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\r 1/32 [..............................] - ETA: 0s - loss: 5024.0854 - accuracy: 0.0312",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r32/32 [==============================] - ETA: 0s - loss: 5044.1157 - accuracy: 0.0990",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r32/32 [==============================] - 0s 2ms/step - loss: 5044.1157 - accuracy: 0.0990\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\r 1/32 [..............................] - ETA: 4s - loss: 6225.1875 - accuracy: 0.2500",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r32/32 [==============================] - 0s 2ms/step - loss: 6145.4878 - accuracy: 0.1040\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Loss: 6145.48779296875\nAccuracy: 0.10400000214576721\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 11
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Natural Language Processing - Tokenizing a Sentence and Creating Vectors\n",
                "\n",
                "Here’s a simple example of Natural Language Processing (NLP) using the NLTK library in Python. This example will tokenize a sentence, which is the process of splitting a sentence into its individual words. It then shows the embeddings of the words. We will use the gensim library’s Word2Vec model for this. Word2Vec is a popular model for embedding words in a high-dimensional vector space.\n",
                "\n",
                "**Imports the NLTK library:** NLTK (Natural Language Toolkit) is a leading platform for building Python programs to work with human language data.\n",
                "\n",
                "**Downloads the Punkt Tokenizer Models:** These are models used by NLTK for tokenizing sentences.\n",
                "\n",
                "**Defines a function to tokenize a sentence:** This function takes a sentence as input and uses the nltk.word\\_tokenize function to split the sentence into its individual words.\n",
                "\n",
                "**Defines a sentence:** This is the sentence that we will tokenize.\n",
                "\n",
                "**Tokenizes the sentence and prints the result:** The tokenize\\_sentence function is called with the sentence as an argument, and the result is printed.\n",
                "\n",
                "**Trains a Word2Vec model:** This model takes the tokenized sentence as input and learns to represent each word as a vector in a high-dimensional space. The min\\_count=1 argument means that all words are included in the model, even those that only appear once.\n",
                "\n",
                "\\*\\*Gets the vector for 'SQL': This is the embedding of the word 'SQL', which the model has learned to represent as a vector.\n",
                "\n",
                "**Prints the vector:** This is the embedding of the word 'SQL'.\n",
                "\n",
                "**Repeats this process:** Now the code repeats the process for the word 'Server', which can be used to plot or use the vector array to find the similarity between these words."
            ],
            "metadata": {
                "azdata_cell_guid": "085d0ae3-e88d-47bb-a8af-f29751ff81a2"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import nltk\r\n",
                "from gensim.models import Word2Vec\r\n",
                "\r\n",
                "def tokenize_sentence(sentence):\r\n",
                "    # Use the NLTK library to tokenize the sentence\r\n",
                "    tokenized_sentence = nltk.word_tokenize(sentence)\r\n",
                "    \r\n",
                "    return tokenized_sentence\r\n",
                "\r\n",
                "# Define a sentence\r\n",
                "sentence = \"Hello, I am a SQL Server developer with 15 years of experience and I'm learning about AI.\"\r\n",
                "\r\n",
                "# Tokenize the sentence\r\n",
                "tokenized_sentence = tokenize_sentence(sentence)\r\n",
                "# Tokenize the sentence and print the result\r\n",
                "print(tokenized_sentence)\r\n",
                "\r\n",
                "# Train a Word2Vec model\r\n",
                "model = Word2Vec([tokenized_sentence], min_count=1)\r\n",
                "\r\n",
                "# Get the vector for 'SQL'\r\n",
                "vector = model.wv['SQL']\r\n",
                "# Print the vector\r\n",
                "print(f\"The vector for 'SQL' is {vector}\")\r\n",
                "\r\n",
                "# Get the vector for 'Server'\r\n",
                "vector = model.wv['Server']\r\n",
                "# Print the vector\r\n",
                "print(f\"And the vector for 'Server' is {vector}\")\r\n",
                ""
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "e4f87205-7688-4e4c-b9d6-f3e51c602741"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "['Hello', ',', 'I', 'am', 'a', 'SQL', 'Server', 'developer', 'with', '15', 'years', 'of', 'experience', 'and', 'I', \"'m\", 'learning', 'about', 'AI', '.']\nThe vector for 'SQL' is [ 8.13227147e-03 -4.45733406e-03 -1.06835726e-03  1.00636482e-03\n -1.91113955e-04  1.14817743e-03  6.11386076e-03 -2.02715401e-05\n -3.24596534e-03 -1.51072862e-03  5.89729892e-03  1.51410222e-03\n -7.24261976e-04  9.33324732e-03 -4.92128357e-03 -8.38409644e-04\n  9.17541143e-03  6.74942741e-03  1.50285603e-03 -8.88256077e-03\n  1.14874600e-03 -2.28825561e-03  9.36823711e-03  1.20992784e-03\n  1.49006362e-03  2.40640994e-03 -1.83600665e-03 -4.99963388e-03\n  2.32429506e-04 -2.01418041e-03  6.60093315e-03  8.94012302e-03\n -6.74754381e-04  2.97701475e-03 -6.10765442e-03  1.69932481e-03\n -6.92623248e-03 -8.69402662e-03 -5.90020278e-03 -8.95647518e-03\n  7.27759488e-03 -5.77203138e-03  8.27635173e-03 -7.24354526e-03\n  3.42167495e-03  9.67499893e-03 -7.78544787e-03 -9.94505733e-03\n -4.32914635e-03 -2.68313056e-03 -2.71289347e-04 -8.83155130e-03\n -8.61755759e-03  2.80021061e-03 -8.20640661e-03 -9.06933658e-03\n -2.34046578e-03 -8.63180775e-03 -7.05664977e-03 -8.40115082e-03\n -3.01328895e-04 -4.56429832e-03  6.62717456e-03  1.52716041e-03\n -3.34147573e-03  6.10897178e-03 -6.01328490e-03 -4.65616956e-03\n -7.20750913e-03 -4.33658017e-03 -1.80932996e-03  6.48964290e-03\n -2.77039292e-03  4.91896737e-03  6.90444233e-03 -7.46370573e-03\n  4.56485013e-03  6.12697843e-03 -2.95447465e-03  6.62502181e-03\n  6.12587947e-03 -6.44348515e-03 -6.76455162e-03  2.53895880e-03\n -1.62381888e-03 -6.06512791e-03  9.49920900e-03 -5.13014663e-03\n -6.55409694e-03 -1.19885204e-04 -2.70142802e-03  4.44400299e-04\n -3.53745813e-03 -4.19330609e-04 -7.08615757e-04  8.22820642e-04\n  8.19481723e-03 -5.73670724e-03 -1.65952800e-03  5.57160750e-03]\nAnd the vector for 'Server' is [ 8.1662899e-03 -4.4389507e-03  8.9885676e-03  8.2520517e-03\n -4.4301399e-03  3.0012726e-04  4.2768861e-03 -3.9214860e-03\n -5.5645299e-03 -6.5142703e-03 -6.7303266e-04 -3.0368229e-04\n  4.4663223e-03 -2.4740603e-03 -1.6827906e-04  2.4597901e-03\n  4.8675709e-03 -2.8712018e-05 -6.3388092e-03 -9.2689516e-03\n  3.1183965e-05  6.6638952e-03  1.4673678e-03 -8.9694476e-03\n -7.9393527e-03  6.5548127e-03 -3.7870058e-03  6.2512243e-03\n -6.6813068e-03  8.4785242e-03 -6.5157544e-03  3.2856434e-03\n -1.0572386e-03 -6.7882822e-03 -3.2865112e-03 -1.1562164e-03\n -5.4722759e-03 -1.2085395e-03 -7.5647580e-03  2.6430010e-03\n  9.0728905e-03 -2.3818812e-03 -9.7622833e-04  3.5205330e-03\n  8.6671989e-03 -5.9236200e-03 -6.8928623e-03 -2.9372128e-03\n  9.1492841e-03  8.6573581e-04 -8.6782426e-03 -1.4466011e-03\n  9.4811693e-03 -7.5522298e-03 -5.3550429e-03  9.3171038e-03\n -8.9717852e-03  3.8244061e-03  6.6743448e-04  6.6640819e-03\n  8.3103683e-03 -2.8532986e-03 -3.9900360e-03  8.9013558e-03\n  2.0900236e-03  6.2517994e-03 -9.4467979e-03  9.5977327e-03\n -1.3464534e-03 -6.0525369e-03  2.9941327e-03 -4.5606564e-04\n  4.7102948e-03 -2.2778395e-03 -4.1340296e-03  2.2812237e-03\n  8.3605777e-03 -4.9961577e-03  2.6691956e-03 -7.9904655e-03\n -6.7776092e-03 -4.6787449e-04 -8.7664090e-03  2.7909223e-03\n  1.5994062e-03 -2.3166582e-03  5.0067143e-03  9.7468216e-03\n  8.4588043e-03 -1.8799815e-03  2.0633438e-03 -3.9992500e-03\n -8.2352534e-03  6.2767519e-03 -1.9455727e-03 -6.5895444e-04\n -1.7687383e-03 -4.5353277e-03  4.0649571e-03 -4.2715878e-03]\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 33
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Large Language Models: A Few Basic Concepts\n",
                "\n",
                "Creating a simple Large Language Model (LLM) from scratch in Python is a complex task that requires a significant amount of computational resources and time. However, we can walk through the high-level steps involved in this process.\n",
                "\n",
                "1. **Data Collection**: Gather a large corpus of text data. This could be books, websites, articles, etc. The more diverse and extensive your dataset, the better your model will be at understanding language.\n",
                "    \n",
                "2. **Preprocessing**: Clean your text data and convert it into a suitable format. This usually involves removing unnecessary characters, converting all text to lowercase, and splitting the text into tokens (words, for example).\n",
                "    \n",
                "3. **Model Definition**: Define your model architecture. For a simple LLM, you might start with a Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) units.\n",
                "    \n",
                "4. **Training**: Train your model on your preprocessed data. This involves feeding your data into the model, having it make predictions, and updating the model's weights based on its errors.\n",
                "    \n",
                "5. **Text Generation**: Use your trained model to generate new text. This involves feeding a seed sequence into your model and having it predict the next token, then updating your seed sequence with this new token and repeating the process.\n",
                "    \n",
                "\n",
                "This code does not include the text generation part, and it's a very simplified version of what a real LLM would look like. It's also worth noting that training such a model on a large dataset would require a significant amount of computational resources and could take a long time."
            ],
            "metadata": {
                "azdata_cell_guid": "bbdcdda3-d2c1-4d48-a07b-3b1c3d6a384d"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "from keras.models import Sequential\r\n",
                "from keras.layers import Dense, LSTM\r\n",
                "from keras.optimizers import RMSprop\r\n",
                "import numpy as np\r\n",
                "\r\n",
                "# This is your raw text\r\n",
                "text = \"A really large body of text called a Corpus would really go here\"\r\n",
                "\r\n",
                "# These are your unique characters in the text\r\n",
                "chars = sorted(list(set(text)))\r\n",
                "\r\n",
                "# Create a dictionary mapping from character to unique index\r\n",
                "char_indices = dict((c, i) for i, c in enumerate(chars))\r\n",
                "\r\n",
                "# Cut the text into sequences and their next characters\r\n",
                "sequences = []\r\n",
                "next_chars = []\r\n",
                "for i in range(0, len(text) - 40, 3):\r\n",
                "    sequences.append(text[i: i + 40])\r\n",
                "    next_chars.append(text[i + 40])\r\n",
                "\r\n",
                "# Vectorize the sequences\r\n",
                "X = np.zeros((len(sequences), 40, len(chars)), dtype=bool)\r\n",
                "y = np.zeros((len(sequences), len(chars)), dtype=bool)\r\n",
                "for i, sequence in enumerate(sequences):\r\n",
                "    for t, char in enumerate(sequence):\r\n",
                "        X[i, t, char_indices[char]] = 1\r\n",
                "    y[i, char_indices[next_chars[i]]] = 1\r\n",
                "\r\n",
                "# Define the model\r\n",
                "model = Sequential()\r\n",
                "model.add(LSTM(128, input_shape=(40, len(chars))))\r\n",
                "model.add(Dense(len(chars), activation='softmax'))\r\n",
                "\r\n",
                "# Compile the model\r\n",
                "optimizer = RMSprop(learning_rate=0.01)\r\n",
                "model.compile(loss='categorical_crossentropy', optimizer=optimizer, run_eagerly=True)\r\n",
                "\r\n",
                "# Train the model\r\n",
                "model.fit(X, y, batch_size=128, epochs=10)\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "29613397-9b01-4fdf-bd30-c1e1e12ceac3",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Epoch 1/10\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\r1/1 [==============================] - ETA: 0s - loss: 3.0517",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 0s 183ms/step - loss: 3.0517\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Epoch 2/10\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\r1/1 [==============================] - ETA: 0s - loss: 2.5710",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 0s 152ms/step - loss: 2.5710\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Epoch 3/10\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\r1/1 [==============================] - ETA: 0s - loss: 2.2408",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 0s 150ms/step - loss: 2.2408\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Epoch 4/10\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\r1/1 [==============================] - ETA: 0s - loss: 3.1882",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 0s 158ms/step - loss: 3.1882\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Epoch 5/10\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\r1/1 [==============================] - ETA: 0s - loss: 2.6095",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 0s 154ms/step - loss: 2.6095\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Epoch 6/10\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\r1/1 [==============================] - ETA: 0s - loss: 2.0750",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 0s 169ms/step - loss: 2.0750\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Epoch 7/10\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\r1/1 [==============================] - ETA: 0s - loss: 1.9064",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 0s 169ms/step - loss: 1.9064\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Epoch 8/10\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\r1/1 [==============================] - ETA: 0s - loss: 1.7473",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 0s 160ms/step - loss: 1.7473\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Epoch 9/10\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\r1/1 [==============================] - ETA: 0s - loss: 1.7007",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 0s 148ms/step - loss: 1.7007\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Epoch 10/10\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\r1/1 [==============================] - ETA: 0s - loss: 1.7281",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 0s 154ms/step - loss: 1.7281\n",
                    "output_type": "stream"
                },
                {
                    "data": {
                        "text/plain": "<keras.src.callbacks.History at 0x1afeb402880>"
                    },
                    "metadata": {},
                    "execution_count": 41,
                    "output_type": "execute_result"
                }
            ],
            "execution_count": 41
        }
    ]
}